{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 3: Transfer Learning with ResNet50 (Google Colab Version)\n",
                "\n",
                "This notebook implements the transfer learning pipeline. It is designed to run on Google Colab, mounting your Google Drive to access the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
                        "TensorFlow Version: 2.19.0\n",
                        "\n",
                        "✅ GPU Detected: 1 found\n",
                        " - Name: /physical_device:GPU:0\n",
                        " - Type: GPU\n",
                        "\n",
                        "--- NVIDIA-SMI Output ---\n",
                        "Sat Dec  6 17:27:58 2025       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
                        "| N/A   30C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|  No running processes found                                                             |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "import sys\n",
                "\n",
                "print(f\"Python Version: {sys.version}\")\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")\n",
                "\n",
                "# Check for GPU\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    print(f\"\\n✅ GPU Detected: {len(gpus)} found\")\n",
                "    for gpu in gpus:\n",
                "        print(f\" - Name: {gpu.name}\")\n",
                "        print(f\" - Type: {gpu.device_type}\")\n",
                "        \n",
                "    # If on Colab/Linux with NVIDIA GPU, this gives more details:\n",
                "    try:\n",
                "        print(\"\\n--- NVIDIA-SMI Output ---\")\n",
                "        !nvidia-smi\n",
                "    except:\n",
                "        print(\"nvidia-smi not found (Expected if on Mac or TPU)\")\n",
                "else:\n",
                "    print(\"\\n❌ No GPU Detected. Running on CPU.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "mount failed",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-699484086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Drive Mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mValueError\u001b[0m: mount failed"
                    ]
                }
            ],
            "source": [
                "# 1. Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print(\"Drive Mounted!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "mount failed",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3329394316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mValueError\u001b[0m: mount failed"
                    ]
                }
            ],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive', force_remount=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Configuration\n",
                "import os\n",
                "\n",
                "# UPDATE THIS PATH to point to your dataset folder in Google Drive\n",
                "# Example: \"/content/drive/MyDrive/Projects/FakeCurrency/Final_Clean_Dataset\"\n",
                "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/Final_Clean_Dataset\"\n",
                "\n",
                "# We copy the dataset to the local Colab VM for speed (reading from Drive is slow)\n",
                "LOCAL_DATASET_PATH = \"/content/Final_Clean_Dataset\"\n",
                "\n",
                "if not os.path.exists(LOCAL_DATASET_PATH):\n",
                "    print(\"Copying dataset to local runtime (this speeds up training)...\")\n",
                "    # If it's a folder, we use cp -r. If it's a zip, we unzip.\n",
                "    # Assuming it is a folder based on your description.\n",
                "    !cp -r \"$DRIVE_DATASET_PATH\" \"$LOCAL_DATASET_PATH\"\n",
                "    print(\"Copy complete.\")\n",
                "else:\n",
                "    print(\"Dataset already exists locally.\")\n",
                "\n",
                "DATASET_ROOT = LOCAL_DATASET_PATH"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Imports & Setup\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.applications import ResNet50\n",
                "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "\n",
                "OUTPUT_DIR = \"/content/Module3_Results\"\n",
                "if not os.path.exists(OUTPUT_DIR):\n",
                "    os.makedirs(OUTPUT_DIR)\n",
                "\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 20\n",
                "LEARNING_RATE = 0.0001"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Data Generators\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    rotation_range=20,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    shear_range=0.2,\n",
                "    zoom_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    fill_mode='nearest',\n",
                "    validation_split=0.2\n",
                ")\n",
                "\n",
                "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
                "\n",
                "print(f\"Loading data from: {DATASET_ROOT}\")\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    DATASET_ROOT,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='binary',\n",
                "    subset='training',\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "validation_generator = val_datagen.flow_from_directory(\n",
                "    DATASET_ROOT,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='binary',\n",
                "    subset='validation',\n",
                "    shuffle=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Build & Train Model\n",
                "def build_model():\n",
                "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
                "    for layer in base_model.layers:\n",
                "        layer.trainable = False\n",
                "    \n",
                "    x = base_model.output\n",
                "    x = GlobalAveragePooling2D()(x)\n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    x = Dropout(0.5)(x)\n",
                "    predictions = Dense(1, activation='sigmoid')(x)\n",
                "    \n",
                "    model = Model(inputs=base_model.input, outputs=predictions)\n",
                "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
                "                  loss='binary_crossentropy',\n",
                "                  metrics=['accuracy'])\n",
                "    return model\n",
                "\n",
                "model = build_model()\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(os.path.join(OUTPUT_DIR, 'model_resnet50_best.h5'), \n",
                "                    monitor='val_loss', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
                "    epochs=EPOCHS,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Evaluation\n",
                "# Plot History\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['accuracy'], label='Train Acc')\n",
                "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
                "plt.title('Accuracy')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.title('Loss')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# Confusion Matrix\n",
                "validation_generator.reset()\n",
                "preds = model.predict(validation_generator, verbose=1)\n",
                "y_pred = (preds > 0.5).astype(int).flatten()\n",
                "y_true = validation_generator.classes\n",
                "\n",
                "print(classification_report(y_true, y_pred, target_names=['Real', 'Fake']))\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
